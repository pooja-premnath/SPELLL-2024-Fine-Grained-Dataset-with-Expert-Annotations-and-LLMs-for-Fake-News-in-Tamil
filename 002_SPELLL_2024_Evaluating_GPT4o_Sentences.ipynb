{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvzu1v1ERYPV",
        "outputId": "7627c7b2-0a12-431f-cfb6-50c3ebf85821"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SPELLL-2024-Fine-Grained-Dataset-with-Expert-Annotations-and-LLMs-for-Fake-News-in-Tamil'...\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 7 (delta 2), reused 7 (delta 2), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (7/7), 192.82 KiB | 19.28 MiB/s, done.\n",
            "Resolving deltas: 100% (2/2), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/pooja-premnath/SPELLL-2024-Fine-Grained-Dataset-with-Expert-Annotations-and-LLMs-for-Fake-News-in-Tamil"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the original and generated datasets\n",
        "original_df = pd.read_csv('/content/SPELLL-2024-Fine-Grained-Dataset-with-Expert-Annotations-and-LLMs-for-Fake-News-in-Tamil/Data/Expert-Annotated Dataset.csv')\n",
        "generated_df = pd.read_csv('/content/SPELLL-2024-Fine-Grained-Dataset-with-Expert-Annotations-and-LLMs-for-Fake-News-in-Tamil/Data/GPT4o Dataset.csv')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GcPopHrfR6-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TTR Ratio-Lexical Diversity"
      ],
      "metadata": {
        "id": "mVHxZHezSYar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def type_token_ratio(text):\n",
        "    tokens = text.split()\n",
        "    return len(set(tokens)) / len(tokens)\n",
        "\n",
        "# Calculate TTR for the entire datasets\n",
        "original_ttr = original_df['Text'].apply(type_token_ratio).mean()\n",
        "generated_ttr = generated_df['Text'].apply(type_token_ratio).mean()\n",
        "\n",
        "# Calculate TTR by category\n",
        "original_ttr_by_category = original_df.groupby('Category')['Text'].apply(lambda x: x.apply(type_token_ratio).mean())\n",
        "generated_ttr_by_category = generated_df.groupby('Category')['Text'].apply(lambda x: x.apply(type_token_ratio).mean())\n",
        "\n",
        "original_ttr, generated_ttr, original_ttr_by_category, generated_ttr_by_category\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHQv9cCAR81l",
        "outputId": "af033b70-cfa4-4f4b-f43b-2f1d21580251"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9981139499407483,\n",
              " 0.9984322871572872,\n",
              " Category\n",
              " Biased        1.000000\n",
              " Clickbait     0.998617\n",
              " Humor         1.000000\n",
              " Misleading    0.997376\n",
              " Name: Text, dtype: float64,\n",
              " Category\n",
              " Biased    0.999775\n",
              " Humor     0.997090\n",
              " Name: Text, dtype: float64)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## N-Gram Overlap"
      ],
      "metadata": {
        "id": "Du99d3OjS8aD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "\n",
        "def ngram_overlap(text, reference_ngrams, n=2):\n",
        "    sentence_ngrams = Counter(ngrams(text.split(), n))\n",
        "    overlap = sum(min(sentence_ngrams[ng], reference_ngrams[ng]) for ng in sentence_ngrams)\n",
        "    return overlap / max(1, sum(sentence_ngrams.values()))\n",
        "\n",
        "# Get bigrams from the original dataset\n",
        "original_ngrams = Counter(ngrams(\" \".join(original_df['Text']).split(), 2))\n",
        "\n",
        "# Calculate n-gram overlap for the entire generated dataset\n",
        "ngram_overlap_scores = generated_df['Text'].apply(ngram_overlap, reference_ngrams=original_ngrams).mean()\n",
        "\n",
        "# Calculate n-gram overlap by category\n",
        "ngram_overlap_by_category = generated_df.groupby('Category')['Text'].apply(lambda x: x.apply(ngram_overlap, reference_ngrams=original_ngrams).mean())\n",
        "\n",
        "ngram_overlap_scores, ngram_overlap_by_category\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mq8HVzlUSC_5",
        "outputId": "b81bc36a-c791-4530-f733-6387fbc12f9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.03201152597402598,\n",
              " Category\n",
              " Biased    0.039308\n",
              " Humor     0.024715\n",
              " Name: Text, dtype: float64)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w04oY0T1URBA",
        "outputId": "b78772e1-6dc2-44bd-c202-ba096e4fd818"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.42.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.3.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence-transformers\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 sentence-transformers-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = SentenceTransformer('paraphrase-xlm-r-multilingual-v1')\n",
        "\n",
        "# Filter the original dataset to include only \"Humor\" and \"Biased\" categories\n",
        "filtered_original_df = original_df[original_df['Category'].isin(['Humor', 'Biased'])]\n",
        "\n",
        "# Ensure that the filtered original dataset has the same order of labels as the generated dataset\n",
        "# Assuming both datasets are aligned or we can sort them based on the category and index\n",
        "filtered_original_df = filtered_original_df.reset_index(drop=True)\n",
        "generated_df = generated_df.reset_index(drop=True)\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = SentenceTransformer('paraphrase-xlm-r-multilingual-v1')\n",
        "\n",
        "# Generate sentence embeddings\n",
        "original_embeddings = model.encode(filtered_original_df['Text'].tolist())\n",
        "generated_embeddings = model.encode(generated_df['Text'].tolist())\n",
        "\n",
        "# Calculate cosine similarity\n",
        "cosine_similarities = [cosine_similarity([orig], [gen])[0][0] for orig, gen in zip(original_embeddings, generated_embeddings)]\n",
        "average_cosine_similarity = sum(cosine_similarities) / len(cosine_similarities)\n",
        "\n",
        "print(f\"Average Cosine Similarity: {average_cosine_similarity}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NruQ3Oo-TamE",
        "outputId": "ea67b51a-8178-4a05-9ba1-dd27dd6786b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Cosine Similarity: 0.2174960909461653\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Specify the device to use the GPU (0 for the first GPU, -1 for CPU)\n",
        "device = 0  # Use GPU; set to -1 if you want to use the CPU\n",
        "\n",
        "# Load the sentiment analysis pipeline with the specified device\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"ai4bharat/indic-bert\", device=device)\n",
        "\n",
        "# Filter the original dataset to include only \"Humor\" and \"Biased\" categories\n",
        "filtered_original_df = original_df[original_df['Category'].isin(['Humor', 'Biased'])]\n",
        "\n",
        "# Ensure that the filtered original dataset and the generated dataset are aligned\n",
        "filtered_original_df = filtered_original_df.reset_index(drop=True)\n",
        "generated_df = generated_df.reset_index(drop=True)\n",
        "\n",
        "# Predict sentiments for the filtered datasets\n",
        "original_sentiments = sentiment_pipeline(filtered_original_df['Text'].tolist())\n",
        "generated_sentiments = sentiment_pipeline(generated_df['Text'].tolist())\n",
        "\n",
        "# Compare sentiments to calculate sentiment consistency\n",
        "sentiment_match = sum(1 for orig, gen in zip(original_sentiments, generated_sentiments) if orig['label'] == gen['label'])\n",
        "sentiment_consistency = sentiment_match / len(original_sentiments)\n",
        "\n",
        "print(f\"Sentiment Consistency: {sentiment_consistency}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilWmFB0MUOnV",
        "outputId": "faf9dbb7-594d-4900-ee0b-e3e001c4c7cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment Consistency: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "def shannon_entropy(text):\n",
        "    tokens = text.split()\n",
        "    token_counts = Counter(tokens)\n",
        "    total_count = sum(token_counts.values())\n",
        "    entropy = -sum((count / total_count) * np.log2(count / total_count) for count in token_counts.values())\n",
        "    return entropy\n",
        "\n",
        "# Calculate entropy for the entire datasets\n",
        "original_entropy = original_df['Text'].apply(shannon_entropy).mean()\n",
        "generated_entropy = generated_df['Text'].apply(shannon_entropy).mean()\n",
        "\n",
        "print(f\"Original Shannon Entropy: {original_entropy}\")\n",
        "print(f\"Generated Shannon Entropy: {generated_entropy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXJISJUEW1XU",
        "outputId": "75db5da2-b57b-4ece-9ca4-fcc83ca079ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Shannon Entropy: 3.0754853020250046\n",
            "Generated Shannon Entropy: 2.566051473490822\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from math import pow\n",
        "\n",
        "def simpson_index(text):\n",
        "    tokens = text.split()\n",
        "    counts = Counter(tokens).values()\n",
        "    N = sum(counts)\n",
        "    simpson = sum((n / N) ** 2 for n in counts)\n",
        "    return 1 - simpson  # Higher value = more diversity\n",
        "\n",
        "original_simpson = original_df['Text'].apply(simpson_index).mean()\n",
        "generated_simpson = generated_df['Text'].apply(simpson_index).mean()\n",
        "\n",
        "print(f\"Original Simpson's Index: {original_simpson}\")\n",
        "print(f\"Generated Simpson's Index: {generated_simpson}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7O0nXM1YOm0",
        "outputId": "b0a679ee-1e17-42ad-eb67-b60b9fc35ac3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Simpson's Index: 0.8767759404930211\n",
            "Generated Simpson's Index: 0.8233121137542513\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def jaccard_similarity(set1, set2):\n",
        "    intersection = len(set1.intersection(set2))\n",
        "    union = len(set1.union(set2))\n",
        "    return intersection / union\n",
        "\n",
        "original_words = set(\" \".join(original_df['Text']).split())\n",
        "generated_words = set(\" \".join(generated_df['Text']).split())\n",
        "\n",
        "similarity = jaccard_similarity(original_words, generated_words)\n",
        "print(f\"Jaccard Similarity: {similarity}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTq9Amb7Xrmx",
        "outputId": "c96f9328-6edf-405d-a491-7c011260417f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jaccard Similarity: 0.08307749558098428\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install language-tool-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbmjS2jTZrrq",
        "outputId": "3e817199-66df-4a02-dad2-e51123ffe8de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting language-tool-python\n",
            "  Downloading language_tool_python-2.8-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (from language-tool-python) (24.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from language-tool-python) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from language-tool-python) (4.66.5)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from language-tool-python) (0.44.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->language-tool-python) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->language-tool-python) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->language-tool-python) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->language-tool-python) (2024.7.4)\n",
            "Downloading language_tool_python-2.8-py3-none-any.whl (35 kB)\n",
            "Installing collected packages: language-tool-python\n",
            "Successfully installed language-tool-python-2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from language_tool_python import LanguageTool\n",
        "\n",
        "tool = LanguageTool('ta')  # 'ta' is the language code for Tamil\n",
        "\n",
        "def check_grammar(sentence):\n",
        "    matches = tool.check(sentence)\n",
        "    return len(matches)  # Number of grammar issues\n",
        "\n",
        "original_grammar_issues = original_df['Text'].apply(check_grammar).mean()\n",
        "generated_grammar_issues = generated_df['Text'].apply(check_grammar).mean()\n",
        "\n",
        "print(f\"Average Grammar Issues in Original Text: {original_grammar_issues}\")\n",
        "print(f\"Average Grammar Issues in Generated Text: {generated_grammar_issues}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UUp43PMZtPU",
        "outputId": "bf3cfea4-c100-4e69-c814-020b4dca45c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading LanguageTool 6.4: 100%|██████████| 246M/246M [00:02<00:00, 85.6MB/s]\n",
            "INFO:language_tool_python.download_lt:Unzipping /tmp/tmpdlxfjfq3.zip to /root/.cache/language_tool_python.\n",
            "INFO:language_tool_python.download_lt:Downloaded https://www.languagetool.org/download/LanguageTool-6.4.zip to /root/.cache/language_tool_python.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Grammar Issues in Original Text: 0.45657809462086846\n",
            "Average Grammar Issues in Generated Text: 0.2345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "85O3yRmBae3o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}